<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yuki M Asano</title>

  <meta name="author" content="Yuki M Asano">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yuki M. Asano</name>
              </p>
              <p style="text-align:center"> Computer Vision | Machine Learning | Complex Systems </p>
              <p>
                I'm a PhD student in the <a href="https://www.robots.ox.ac.uk/~vgg/">Visual Geometry Group (VGG)</a> working with <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a> and <a href="https://chrirupp.github.io/">Christian Rupprecht</a> at the <a href="http://www.ox.ac.uk/">University of Oxford</a>. Prior to this I studied physics at the <a href="http://www.en.uni-muenchen.de/index.html">University of Munich (LMU)</a> and <a href="https://www.fernuni-hagen.de/">Economics in Hagen</a> as well as a <a href="https://www.maths.ox.ac.uk/members/students/postgraduate-courses/msc-mmsc">MSc in Mathematical Modelling and Scientific Computing</a> at the Mathematical Institute in Oxford. Also, I love running, the mountains ⛰️ and their combination.
              </p>
              <p style="text-align:center">
                <a href="mailto:yuki@robots.ox.ac.uk">Email</a> &nbsp/&nbsp
                <!-- <a href="data/YukiMAsano-CV.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.co.uk/citations?user=CdpLhlgAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/yukimasano/">Github</a>&nbsp/&nbsp
                <a href="https://twitter.com/y_m_asano">Twitter</a>&nbsp/&nbsp
                <a href="https://www.linkedin.com/in/yuki-m-asano">LinkedIn</a>&nbsp/&nbsp
                <a href="data/Asano_CV.pdf">CV</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/yuki.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yuki.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li>Our new preprint on intersectional occupational biases of GPT-2 is <a href=http://arxiv.org/abs/2102.04130>out </a> </li>
                  <li>Our paper on video-text representation learning got accepted as a Spotlight into ICLR 2021!</li>
                  <li>I've started volunteering my time at OxAI to help interdisciplinary teams work on AI projects.</li>
                  <li>Our paper on Self-Labelling Videos (SeLaVi) was accepted as a paper to NeurIPS! <a href="https://github.com/facebookresearch/selavi">Code</a> </li>
                  <li>Starting my summer internship June 22nd  at FAIR and working with Armand Joulin and Ishan Misra. </li>
                  <li>I am Co-PI on a Amazon Machine Learning Award project with Christian Rupprecht and Andrea Vedaldi. </li>
                  <li>Awarded with the 2020 <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/2020-europe"> Qualcomm Innovation Fellowship</a>.</li>
                  <li>I'll be co-organizing a workshop at ECCV 2020: <a href="https://sslwin.org/">Self-Supervised Learning: What Is Next?</a> </li>
                  <li>Two papers have been accepted into ICLR 2020 (incl. one Spotlight)</li>
                  <!-- <li>Started using a new homepage!</li> -->
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, complex systems and data. More specifically, I want to understand the necessity and scope of prior knowledge and supervision for good neural networks. To this effect, I work with self-supervised learning and try to understand what makes things work. I'm excited about what we can learn from data alone, from data augmentation, and videos.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/avdet.jpg" alt="Detecting objects without supervision" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2104.06401">
                <papertitle>Self-supervised object detection from audio-visual correspondence</papertitle>
              </a>
              <br>
              <a href=https://www.robots.ox.ac.uk/~afourast/>https://www.robots.ox.ac.uk/~afourast/* </a>, <strong>Yuki M. Asano*</strong>, Francois Fagan, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>, Florian Metze
              <br>
              <br>
              <a href="data/afouras2021selfsupervised.bib">bibtex</a>
              <p>We detect objects without any supervisory signal by leveraging multi-modal signals from videos and combining self-supervised contrastive- and clustering-based learning. Our model learns from video and detects objects in images, without the need for audio during inference.
            </td>
           </tr>
  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gpt-2.png" alt="predictions vs ground-truth (US)" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2102.04130">
                <papertitle>How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases </papertitle>
              </a>
              <br>
              <a href=https://www.hannahrosekirk.com/>Hannah Kirk* </a>, Yennie Jun*, Haider Iqbal*, Elias Benussi*, Filippo Volpin*, Frederic A. Dreyer*, Aleksandar Shtedritski, <strong>Yuki M. Asano*</strong>
              <br>
              <br>
              <a href="data/kirk2021how.bib">bibtex</a>
              <p>We analyze the biases and distributions of GPT-2's output w.r.t. to occupations. Especially interesting as AI find its way into hiring and automated application assessments.
            </td>
           </tr>
           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/support_set.png" alt="Schematic of our method" width="160" height="160">
            </td>
              <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2010.02824">
                 <papertitle>Support-set bottlenecks for video-text representation learning</papertitle>
              </a>
              <br>
              Mandela Patrick*, <a href="http://www.cs.cmu.edu/~poyaoh/">Po-Yao Huang*</a>, <strong>Yuki M. Asano*</strong>, Florian Metze, Alexander Hauptmann, <a href="http://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICLR </em>, 2021 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
              <br>
              <br>
              <a href="data/patrick2020supportset.bib">bibtex</a>
              <p>We use a generative objective to improve the instance discrimination limitations of contrastive learning to set new state-of-the-art results in text-to-video retrieval.
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/labelling_videos.gif" alt="clustered videos" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="http://arxiv.org/abs/2006.13662">
                <papertitle>Labelling unlabelled videos from scratch with multi-modal self-supervision</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano*</strong>, Mandela Patrick*, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>NeurIPS </em>, 2020
              <br>
              <br>
               <a href="https://github.com/facebookresearch/selavi">code |</a> <a href="https://www.robots.ox.ac.uk/~vgg/research/selavi/">homepage |</a> <a href="data/asano2020labelling.bib">bibtex</a>
              <p>Unsupervisedly clustering videos via self-supervision. We show clustering videos well does not come for free from good representations. Instead, we learn a multi-modal clustering function that treats the audio and visual-stream as augmentations.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gdt.png" alt="hierarchical transformations" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2003.04298">
                <papertitle>Multi-modal Self-Supervision from Generalized Data Transformations</papertitle>
              </a>
              <br>
              Mandela Patrick*<strong>, Yuki M. Asano*</strong>, <a href="https://scholar.google.com/citations?user=JdEJEicAAAAJ&hl=en&oi=ao">Polina Kuznetsova</a>, <a href="http://ruthcfong.github.io/">Ruth Fong</a>, <a href="http://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>, Geoffrey Zweig, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <!-- <br> -->
              <!-- <em>ICLR </em>, 2020 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font> -->
              <br>
              <br>
              <a href="data/patrick2020multimodal.bib">bibtex</a>
              <p>We give transformations the prominence they deserve, by introducing a systematic framework suitable for contrastive learning. SOTA video representation learning by learning (in)variances systematically.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/self-clusters.jpg" alt="learned clusters" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/research/self-label/asset/iclr2020_self-label.pdf">
                <papertitle>Self-labelling via simultaneous clustering and representation learning</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano</strong>, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICLR </em>, 2020 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
              <br>
              <a href="https://github.com/yukimasano/self-label">code | </a> <a href="http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html">blog | </a> <a href="data/asano2020self.bib">bibtex</a> | <a href="https://iclr.cc/virtual_2020/poster_Hyx-jyBFPr.html">ICLR presentation</a>
              <p>We propose a self-supervised learning formulation that simultaneously learns feature representations and useful dataset labels by optimizing the common cross-entropy loss for features <i>  and </i> labels, while maximizing information.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ameyoko_crop.jpg" alt="ameyoko" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.13132">
                <papertitle>A critical analysis of self-supervision, or what we can learn from a single image</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano</strong>, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICLR </em>, 2020
              <br>
              <a href="data/asano2020a.bib">bibtex</a> | <a href="https://iclr.cc/virtual_2020/poster_B1esx6EYvr.html">ICLR presentation</a>
              <p>We evaluate self-supervised feature learning methods and find that with sufficient data augmentation early layers can be learned using just one image.  This is informative about self-supervision and the role of augmentations.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/recipes.png" alt="recipes" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.nature.com/articles/s41893-019-0316-0">
                <papertitle>Rising adoption and retention of meat-free diets in online recipe data</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano*</strong> and  <a href="https://www.cdtm.de/cdtm_team/gesa-biermann/">Gesa Biermann*</a>
              <br>
              <em>Nature Sustainability </em>, 2019
              <br>
              <a href="https://github.com/yukimasano/sustainable_recipes">code</a> | <a href="data/asano2019rising.bib">bibtex</a>
              <p>We investigate dietary transitions by analysing a large scale dataset of recipes and user ratings. We detect a consistent increase in the number of users switching to vegetarian diets, and maintaining them. We show that the transition is eased by initially switching to vegetarian diets  </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ramsey.png" alt="ramsey model" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1907.02155.pdf">
                <papertitle>Emergent inequality and endogenous dynamics in a simple behavioral macroeconomic model</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano</strong>, <a href="https://www.pik-potsdam.de/members/kolb">Jakob J. Kolb</a>, <a href="https://www.pik-potsdam.de/members/heitzig">Jobst Heitzig</a>, <a href="https://www.inet.ox.ac.uk/people/j-doyne-farmer/">J. Doyne Farmer</a>
              <br>
              <em>ArXiv </em>, 2019
              <br>
              <a href="data/asano2019emergent.bib">bibtex</a>
              <p>We build an agent-based version of a fundamental macroeconomic model and include simple decision making heuristics. We find highly complex behavior and business cycles.</tr>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/protonCT.jpg" alt="protonCT" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://aapm.onlinelibrary.wiley.com/doi/abs/10.1118/1.4924232">
              <papertitle>Monte Carlo Study of the Precision and Accuracy of Proton CT Reconstructed Relative Stopping Power Maps</papertitle>
              </a>
              <br> G. Dedes, <strong>YM. Asano</strong>, N. Arbor, D. Dauvergne, J. Letang, E. Testa, S. Rit, K. Parodi
              <br>
              <em>Medical Physics </em>, 2016
              <br>
              <a href="data/dedes2015j.bib">bibtex</a>
              <p> In my BSc thesis, I investigated how we can model proton computation tomography (pCT) using Monte-Carlo based software. We simulated an ideal pCT scanner and scans of several cylindrical phantoms with various tissue equivalent inserts of different sizes.
            </td>
          </tr>


        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Other activities</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle"><img src="images/180dcm.jpg"></td>
            <td width="55%" valign="center">
              In Munich, I was the founder and president of a student-run management consultancy for non-profits, <a href="https://www.180dcmunich.org/">180DC Munich</a>. With great interdisciplinary colleagues, we have already helped more than 30 NGOs improve their impact measurement and effectivity.
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <img src="images/int.jpg" alt="internships">
            </td>
            <td width="55%" valign="center">
              I am a curious person.
              <br>
              I got the chance to gain some valuable experiences in consulting and more recently in the technology sector, including internships at Facebook AI Research and Transferwise.
              <br>
              More to come.
            </a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Great template from <a href="https://jonbarron.info/"> Jon Barron</a>
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
