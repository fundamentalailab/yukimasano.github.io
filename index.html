<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yuki M Asano</title>

  <meta name="author" content="Yuki M Asano">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yuki M. Asano</name>
              </p>
              <p style="text-align:center"> Computer Vision | Machine Learning | Complex Systems </p>
              <p>
                I'm an assistant professor for computer vision and machine learning and Science Manager of <a href="https://ivi.fnwi.uva.nl/quva/">QUVA lab</a> at the University of Amsterdam, where I work with <a href="https://www.ceessnoek.info/">Cees Snoek</a>, <a href="https://staff.fnwi.uva.nl/m.welling/">Max Welling</a> and <a href="https://www.egavves.com/">Efstratios Gavves</a>.
                My PhD was at the <a href="https://www.robots.ox.ac.uk/~vgg/">Visual Geometry Group (VGG)</a> at the University of Oxford where I worked with <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a> and <a href="https://chrirupp.github.io/">Christian Rupprecht</a>. Prior to this I studied physics at the <a href="http://www.en.uni-muenchen.de/index.html">University of Munich (LMU)</a> and <a href="https://www.fernuni-hagen.de/">Economics in Hagen</a> as well as a <a href="https://www.maths.ox.ac.uk/members/students/postgraduate-courses/msc-mmsc">MSc in Mathematical Modelling and Scientific Computing</a> at the Mathematical Institute in Oxford. Also, I love running, the mountains ⛰️ and their combination.
              </p>
              <p style="text-align:center">
                <a href="mailto:yukiATMARKrobots.ox.ac.uk">Email</a> &nbsp/&nbsp
                <!-- <a href="data/YukiMAsano-CV.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.co.uk/citations?user=CdpLhlgAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/yukimasano/">Github</a>&nbsp/&nbsp
                <a href="https://twitter.com/y_m_asano">Twitter</a>&nbsp/&nbsp
                <a href="https://www.linkedin.com/in/yuki-m-asano">LinkedIn</a>&nbsp/&nbsp
                <a href="data/Asano_CV.pdf">CV</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/yuki.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yuki.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li>Welcome to my new MSc thesis students: Lukas, Luc, Apostolos and Alfonso!</li>
                  <li>I'm honored to serve as an Area Chair for CVPR'23! </li>
                  <li>If you are a MSc in AI student at the UvA and want to write your thesis with me, please contact me. I have some exciting projects. </li>
                  <li>Two papers accepted at ECCV'22! </li>
                  <li>One paper accepted at ICML'22! </li>
                  <li>I'm honored to serve as an Area Chair for ECCV'22! </li>
                  <li>Two papers accepted at CVPR'22! </li>
                  <li>One paper accepted at ICLR'22! </li>
                  <li>Qualcomm UvA Deep Vision public seminar talk in Dec. 2021. <a href=https://www.youtube.com/watch?v=6XjDDNCG5iA>link</a></li>
                  <li>New <a href="https://arxiv.org/abs/2112.00725">preprint</a> on single-image learning.</li>
                  <li>Starting as an Assistant Professor at the UvA from Oct 2021. </li>
                  <li>Two papers accepted at NeurIPS'21 (including the first as supervisor) </li>
                  <li>One paper accepted at NeurIPS'21-Datasets Track: the <a href=https://www.robots.ox.ac.uk/~vgg/research/pass/>PASS dataset</a>, incl. <a href=https://github.com/yukimasano/PASS/>pretrained models</a>. </li>
                  <li>Passed my PhD with "no corrections", my examiners were Phillip Isola and Philip Torr. </li>
                  <li>Two papers accepted to ICCV'21! (GDT and STiCA) </li>
                  <li>One paper I supervised accepted to ACL'21's Workshop on online abuse and harms. More details to follow. </li>
                  <li>One paper accepted to <a href=https://www.pnas.org/content/118/27/e2025721118>PNAS</a>, my Erdös Number is now 3 via Jobst Heitzig. </li>
                  <li>New preprint: using clustering & contrastive SSL we find objects <a href=https://arxiv.org/abs/2104.06401>without any supervision </a> </li>
                  <li><input type="checkbox" id="toggle" class="unfolder"/>
                  <label for="toggle" class="toggle-label"><span class="fold-icon">&#9660;</span>More..</label>
                  <div class="fold">
                  <ul>
                    <li>OxAI team I supervised has published its results at ICLR'21 <a href=https://sdg-quality-privacy-bias.github.io/papers/>SDG workshop  </a> </li>
                    <li>Our new preprint on intersectional occupational biases of GPT-2 is <a href=http://arxiv.org/abs/2102.04130>out </a> </li>
                    <li>Our paper on video-text representation learning got accepted as a Spotlight into ICLR 2021!</li>
                    <li>I've started volunteering my time at OxAI to help interdisciplinary teams work on AI projects.</li>
                    <li>Our paper on Self-Labelling Videos (SeLaVi) was accepted as a paper to NeurIPS! <a href="https://github.com/facebookresearch/selavi">Code</a> </li>
                    <li>Starting my summer internship June 22nd  at FAIR and working with Armand Joulin and Ishan Misra. </li>
                    <li>I am Co-PI on a Amazon Machine Learning Award project with Christian Rupprecht and Andrea Vedaldi. </li>
                    <li>Awarded with the 2020 <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/2020-europe"> Qualcomm Innovation Fellowship</a>.</li>
                    <li>I'll be co-organizing a workshop at ECCV 2020: <a href="https://sslwin.org/">Self-Supervised Learning: What Is Next?</a> </li>
                    <li>Two papers have been accepted into ICLR 2020 (incl. one Spotlight)</li>
                  </ul>
                </li>
                  </div>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Teaching</heading>
            <p>
              I'm teaching the <a href='https://uvadlc.github.io/'>Deep Learning Course</a> for the MSc in AI at the University of Amsterdam.
            </p>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
     

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, self-supervised and multi-modal learning as well as privacy and ethics in AI.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
       
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                     <img src="images/prompt.png" alt="method" width="160" height="100">
                   </td>
                   <td width="75%" valign="middle">
                     <a href="https://arxiv.org/abs/2210.06466">
                       <papertitle>Prompt Generation Networks for Efficient Adaptation of Frozen Vision Transformers</papertitle>
                     </a>
                     <br>
                     Jochem Loedeman, Maarten C. Stol, <a href="https://tengdahan.github.io/">Tengda Han</a>, <strong>Yuki M. Asano</strong>
                     <br>
                     <em>arXiv</em> 2022
                     <br>
                     <a href="data/Loedeman2022prompt.bib">bibtex</a>
                     <p> We propose to adapt frozen vision transformers by providing input-dependent prompts, computed by a light-weight network. We surpass linear- & full-finetuning in multiple benchmarks.
                   </td>
           </tr> 

           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                     <img src="images/sgd.png" alt="method" width="160" height="100">
                   </td>
                   <td width="75%" valign="middle">
                     <a href="https://arxiv.org/abs/2210.06462">
                       <papertitle>Self-Guided Diffusion Models </papertitle>
                     </a>
                     <br>
                     Vincent Tao Hu, David W Zhang, <strong>Yuki M. Asano</strong>, Gertjan J. Burghouts, Cees G. M. Snoek
                     <br>
                     <em>arXiv</em> 2022
                     <br>
                     <a href="data/Hu2022self.bib">bibtex</a>
                     <p> We propose to use self-supervision to provide diffusion models a guidance signal, this works better than label guidance.
                   </td>
           </tr> 




          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                     <img src="images/vtc.png" alt="method" width="160" height="100">
                   </td>
                   <td width="75%" valign="middle">
                     <a href="https://unitaryai.github.io/vtc-paper/assets/VTC_paper.pdf">
                       <papertitle>VTC: Improving Video-Text Retrieval with User Comments</papertitle>
                     </a>
                     <br>
                     Laura Hanu, <strong>Yuki M. Asano</strong>, James Thewlis, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>
                     <br>
                     <em>ECCV</em> 2022
                     <br>
                     <a href="data/hanu2022vtc.bib">bibtex</a>
                     <p> We propose to utlize the "comments" modality which is common for internet data and show that it can improve vision-language learning.
                   </td>
           </tr>     

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                     <img src="images/less.jpg" alt="method" width="130" height="130">
                   </td>
                   <td width="75%" valign="middle">
                     <a href="https://arxiv.org/abs/2204.08874">
                       <papertitle>Less than Few: Self-Shot Video Instance Segmentation</papertitle>
                     </a>
                     <br>
                     Pengwan Yang, <strong>Yuki M. Asano</strong>, <a href="https://staff.fnwi.uva.nl/p.s.m.mettes/">Pascal Mettes</a>, <a href="https://www.ceessnoek.info/">Cees G. M. Snoek</a>
                     <br>
                     <em>ECCV</em> 2022
                     <br>
                     <a href="data/yang2022less.bib">bibtex</a>
                     <p> We propose to tackle the task of video instance segmentation by leveraging self-supervised learning to generate support samples at inference time for improved performances.
                   </td>
           </tr>   
          <tr>

       <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/gpt3.png" alt="method" width="100">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2205.11374">
                  <papertitle>Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements</papertitle>
                </a>
                <br>
                Conrad Borchers, Dalia Sara Gala, Benjamin Gilburt, Eduard Oravkin, Wilfried Bounsi, <strong>Yuki M. Asano</strong>, Hannah Kirk
                <br>
                <em>Workshop on Gender Bias in Natural Language Processing at NAACL 2022</em> &nbsp <font color=#FF8080><strong>(Oral)</strong></font>, 2022
                <br>
                <a href="data/borchers2022looking.bib">bibtex</a>
                <p> We investigate bias and mitigation strategies when using GPT-3 for generating job-advertisements.
              </td>
            </tr>       
          
          
       <tr>
       <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/citris.png" alt="method" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2202.03169">
                  <papertitle>CITRIS: Causal Identifiability from Temporal Intervened Sequences </papertitle>
                </a>
                <br>
                <a href="https://phlippe.github.io/">Phillip Lippe</a>, <a href="https://smaglia.wordpress.com/">Sara Magliacane</a>, <a href="https://loewex.github.io/">Sindy Löwe</a>, <strong>Yuki M. Asano</strong>, <a href="https://tacocohen.wordpress.com/">Taco Cohen</a>, <a href='https://www.egavves.com/'></a>Efstratios Gavves</a>
                <br>
                <em>ICML</em>, 2022
                <br>
                <a href="data/lippe2022citris.bib">bibtex</a>
                <p> We do visual causal representation learning using videos. Our method is able to identify causal variables by intervening on them and observing their effects in time.
              </td>
            </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/leopart.png" alt="Unsupervised segmentation performance" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2204.13101">
                <papertitle>Self-Supervised Learning of Object Parts for Semantic Segmentation</papertitle>
              </a>
              <br>
              Adrian Ziegler, <strong>Yuki M. Asano</strong>
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="data/ziegler2022leopart.bib">bibtex</a>
              <p>We self-supervisedly learn how to detect objects by learning to detect and combine self-segmented object parts starting from SSL pretrained ViTs.
            </td>
           </tr>
          <tr>
             
             
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/avdet.jpg" alt="Detecting objects without supervision" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2104.06401">
                <papertitle>Self-supervised object detection from audio-visual correspondence</papertitle>
              </a>
              <br>
              <a href=https://www.robots.ox.ac.uk/~afourast/>Triantafyllos Afouras* </a>, <strong>Yuki M. Asano*</strong>, Francois Fagan, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>, Florian Metze
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="data/afouras2021selfsupervised.bib">bibtex</a>
              <p>We detect objects without any supervisory signal by leveraging multi-modal signals from videos and combining self-supervised contrastive- and clustering-based learning. Our model learns from video and detects objects in images.
            </td>
           </tr>
           <tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/reverse_probing.png" alt="method" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openreview.net/forum?id=HFPTzdwN39">
                  <papertitle>Measuring the Interpretability of Unsupervised Representations via Quantized Reversed Probing </papertitle>
                </a>
                <br>
                Iro Laina, <strong>Yuki M. Asano</strong></a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
                <br>
                <em>ICLR</em>, 2022
                <br>
                <a href="data/laina2022measuring.bib">bibtex</a>
                <p> We propose quantized reverse probing as a information-theoretic measure to assess the degree to which self-supervised visual representations align with human-interpretable concepts.  his measure is also able to detect when the representation correlates with combinations of labelled concepts (e.g. "red apple") instead of just individual attributes ("red" and "apple" separately).
              </td>
            </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/kd.gif" alt="method" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.00725">
                <papertitle>Extrapolating from a Single Image to a Thousand Classes using Distillation</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano</strong>*</a>, <a href="http://aqibsaeed.github.io">Aaqib Saeed*</a>
              <br>
              <em>arxiv</em>, 2021
              <br>
              <a href="https://single-image-distill.github.io">website | <a href="https://github.com/yukimasano/single-img-extrapolating">code | <a href="data/asaon2021extrapolating.bib">bibtex</a>
              <p> We show that it is possible to extrpolate to semantic classes such as those of ImageNet using just a single datum as visual inputs. We leverage knowledge distillation for this and achieve performances of 94%/74% on CIFAR-10/100, 59% on ImageNet and, by extending this method to audio, 84% on SpeechCommands.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/motionformer.png" alt="trajectory attention" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2106.05392">
                <papertitle>Keeping Your Eye On the Ball: Trajectory Attention in Video Transformers</papertitle>
              </a>
              <br>
              <a href="https://mandelapatrick.github.io/">Mandela Patrick*</a>, <a href="https://sites.google.com/view/djcampbell">Dylan Campbell*</a>, <a href="https://yukimasano.github.io/"><strong>Yuki M. Asano</strong>*</a>, <a href="https://imisra.github.io/">Ishan Misra</a>, <a href="https://www.cs.cmu.edu/~fmetze/interACT/Home.html">Florian Metze</a>, <a href="https://feichtenhofer.github.io/">Christoph Feichtenhofer</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>, <a href="http://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>
              <br>
              <em>NeurIPS</em>, 2021 &nbsp <font color=#FF8080><strong>(Oral)</strong></font>
              <br>
              <a href="https://github.com/facebookresearch/Motionformer">code | <a href="data/patrick2021motionformer.bib">bibtex</a>
              <p>We present <i>trajectory attention</i>, a drop-in self-attention block for video transformers that implicitly tracks space-time patches along motion paths. We set SOTA results on a number of action recognition datasets: Kinetics-400, Something-Something V2, and Epic-Kitchens.
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gpt-2.png" alt="predictions vs ground-truth (US)" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2102.04130">
                <papertitle>Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models </papertitle>
              </a>
              <br>
              <a href=https://www.hannahrosekirk.com/>Hannah Kirk </a>, Yennie Jun, Haider Iqbal, Elias Benussi, Filippo Volpin, Frederic A. Dreyer, Aleksandar Shtedritski, <strong>Yuki M. Asano</strong>
              <br>
              <em>NeurIPS</em>, 2021 &nbsp
              <br>
              <a href="https://github.com/oxai/intersectional_gpt2">code | <a href="data/kirk2021bias.bib">bibtex</a>
              <p>We analyze the biases and distributions of GPT-2's output w.r.t. to occupations. Especially interesting as AI find its way into hiring and automated application assessments.
            </td>
           </tr>
           <tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pass.png" alt="the pass dataset" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/forum?id=BwzYI-KaHdr">
                <papertitle>PASS: An ImageNet replacement for self-supervised pretraining without humans.</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano</strong>, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>, <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>NeurIPS Datasets and Benchmarks</em>, 2021 &nbsp </font>
              <br>
              <a href="https://www.robots.ox.ac.uk/~vgg/research/pass/">webpage | <a href="https://zenodo.org/record/6615455">data</a> | <a href="data/asano2021pass.bib">bibtex</a> | <a href="https://github.com/yukimasano/PASS/">pretrained models</a>
              <p>We introduce PASS, a large-scale image dataset that does not include any humans, and show that it can be used for high-quality model pretraining while significantly reducing privacy concerns.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/stica.png" alt="crops help training speed" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2103.10211">
                <papertitle>Space-Time Crop & Attend: Improving Cross-modal Video Representation Learning.</papertitle>
              </a>
              <br>
              Mandela Patrick*<strong>, Yuki M. Asano*</strong>, <a href="https://scholar.google.com/citations?user=JdEJEicAAAAJ&hl=en&oi=ao">Bernie Huang*</a>, Ishan Misra, Florian Metze, <a href="http://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICCV</em>, 2021 &nbsp </font>
              <br>
              <a href="https://github.com/facebookresearch/GDT">code | <a href="data/patrick2021spacetime.bib">bibtex</a>
              <p>We better leverage latent time and space for video representation learning by computing efficient multi-crops in embedding space and using a shallow transformer to model time. This yields SOTA performance and allows for training with longer videos.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gdt.png" alt="hierarchical transformations" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2003.04298">
                <papertitle>On Compositions of Transformations in Contrastive Self-Supervised Learning</papertitle>
              </a>
              <br>
              Mandela Patrick*<strong>, Yuki M. Asano*</strong>, <a href="https://scholar.google.com/citations?user=JdEJEicAAAAJ&hl=en&oi=ao">Polina Kuznetsova</a>, <a href="http://ruthcfong.github.io/">Ruth Fong</a>, <a href="http://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>, Geoffrey Zweig, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICCV</em>, 2021 &nbsp </font>
              <br>
              <a href="https://github.com/facebookresearch/GDT">code | <a href="data/patrick2020multimodal.bib">bibtex</a>
              <p>We give transformations the prominence they deserve by introducing a systematic framework suitable for contrastive learning. SOTA video representation learning by learning (in)variances systematically.
            </td>
          </tr>


           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ramsey.png" alt="ramsey model" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.pnas.org/content/118/27/e2025721118">
                <papertitle>Emergent inequality and business cycles in a simple behavioral macroeconomic model</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano</strong>, <a href="https://www.pik-potsdam.de/members/kolb">Jakob J. Kolb</a>, <a href="https://www.pik-potsdam.de/members/heitzig">Jobst Heitzig</a>, <a href="https://www.inet.ox.ac.uk/people/j-doyne-farmer/">J. Doyne Farmer</a>
              <br>
              <em>Proceedings of the National Academy of Sciences (PNAS)</em>, 2021
              <br>
              <a href="https://github.com/yukimasano/rck_abm">code | <a href="data/asano2021emergent.bib">bibtex</a>
              <p>We build an agent-based version of a fundamental macroeconomic model and include simple decision making heuristics. We find highly complex behavior and business cycles.</tr>
          </tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/privacy.png" alt="Schematic of our method" width="160" height="160">
            </td>
              <td width="75%" valign="middle">
              <a href="https://sdg-quality-privacy-bias.github.io/papers/SDG_paper_4.pdf">
                 <papertitle>Privacy-preserving object detection</papertitle>
              </a>
              <br>
              Peiyang He, Charlie Griffin, Krzysztof Kacprzyk, Artjom Joosen, Michael Collyer, Aleksandar Shtedritski, <strong>Yuki M. Asano</strong>
              <br>
              <em>ICLR</em>, 2021 SGD workshop
              <br>
              <a href="data/he2021privacypreserving.bib">bibtex</a>
              <p>We evaluate the potential of conducting object detection with blurred and GAN-swapped faces. It works well and can potentially even alleviate biases.
            </td>
          </tr>


            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/support_set.png" alt="Schematic of our method" width="160" height="160">
            </td>
              <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2010.02824">
                 <papertitle>Support-set bottlenecks for video-text representation learning</papertitle>
              </a>
              <br>
              Mandela Patrick*, <a href="http://www.cs.cmu.edu/~poyaoh/">Po-Yao Huang*</a>, <strong>Yuki M. Asano*</strong>, Florian Metze, Alexander Hauptmann, <a href="http://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICLR</em>, 2021 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
              <br>
              <a href="data/patrick2020supportset.bib">bibtex</a> | <a href="https://slideslive.com/38953567/supportset-bottlenecks-for-videotext-representation-learning?ref=speaker-42650-latest">talk</a>
              <p>We use a generative objective to improve the instance discrimination limitations of contrastive learning to set new state-of-the-art results in text-to-video retrieval.
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/labelling_videos.gif" alt="clustered videos" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="http://arxiv.org/abs/2006.13662">
                <papertitle>Labelling unlabelled videos from scratch with multi-modal self-supervision</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano*</strong>, Mandela Patrick*, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>NeurIPS</em>, 2020
              <br>
               <a href="https://github.com/facebookresearch/selavi">code |</a> <a href="https://www.robots.ox.ac.uk/~vgg/research/selavi/">homepage |</a> <a href="data/asano2020labelling.bib">bibtex</a> | <a href="https://slideslive.com/38955434/selavi-selflabelling-videos-without-any-annotations-from-scratch?ref=account-84503-latest"> talk</a>
              <p>Unsupervisedly clustering videos via self-supervision. We show clustering videos well does not come for free from good representations. Instead, we learn a multi-modal clustering function that treats the audio and visual-stream as augmentations.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/self-clusters.jpg" alt="learned clusters" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/research/self-label/asset/iclr2020_self-label.pdf">
                <papertitle>Self-labelling via simultaneous clustering and representation learning</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano</strong>, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICLR</em>, 2020 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
              <br>
              <a href="https://github.com/yukimasano/self-label">code | </a> <a href="http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html">blog | </a> <a href="data/asano2020self.bib">bibtex</a> | <a href="https://iclr.cc/virtual_2020/poster_Hyx-jyBFPr.html">ICLR talk</a>
              <p>We propose a self-supervised learning formulation that simultaneously learns feature representations and useful dataset labels by optimizing the common cross-entropy loss for features <i>  and </i> labels, while maximizing information.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ameyoko_crop.jpg" alt="ameyoko" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.13132">
                <papertitle>A critical analysis of self-supervision, or what we can learn from a single image</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano</strong>, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICLR</em>, 2020
              <br>
              <a href="data/asano2020a.bib">bibtex</a> | <a href="https://github.com/yukimasano/single_img_pretraining">code</a> | <a href="https://iclr.cc/virtual_2020/poster_B1esx6EYvr.html">ICLR talk</a>
              <p>We evaluate self-supervised feature learning methods and find that with sufficient data augmentation early layers can be learned using just one image.  This is informative about self-supervision and the role of augmentations.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/recipes.png" alt="recipes" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.nature.com/articles/s41893-019-0316-0">
                <papertitle>Rising adoption and retention of meat-free diets in online recipe data</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano*</strong> and  <a href="https://www.cdtm.de/cdtm_team/gesa-biermann/">Gesa Biermann*</a>
              <br>
              <em>Nature Sustainability </em>, 2019
              <br>
              <a href="https://www.researchgate.net/profile/Gesa-Biermann/publication/334153962_Rising_adoption_and_retention_of_meat-free_diets_in_online_recipe_data/links/5e8c9183a6fdcca789fd0144/Rising-adoption-and-retention-of-meat-free-diets-in-online-recipe-data.pdf">PDF </a> | <a href="https://github.com/yukimasano/sustainable_recipes">code</a> | <a href="data/asano2019rising.bib">bibtex</a>
              <p>We investigate dietary transitions by analysing a large scale dataset of recipes and user ratings. We detect a consistent increase in the number of users switching to vegetarian diets, and maintaining them. We show that the transition is eased by initially switching to vegetarian diets  </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/protonCT.jpg" alt="protonCT" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://aapm.onlinelibrary.wiley.com/doi/abs/10.1118/1.4924232">
              <papertitle>Monte Carlo Study of the Precision and Accuracy of Proton CT Reconstructed Relative Stopping Power Maps</papertitle>
              </a>
              <br> G. Dedes, <strong>YM. Asano</strong>, N. Arbor, D. Dauvergne, J. Letang, E. Testa, S. Rit, K. Parodi
              <br>
              <em>Medical Physics</em>, 2016
              <br>
              <a href="data/dedes2015j.bib">bibtex</a>
              <p> In my BSc thesis, I investigated how we can model proton computation tomography (pCT) using Monte-Carlo based software. We simulated an ideal pCT scanner and scans of several cylindrical phantoms with various tissue equivalent inserts of different sizes.
            </td>
          </tr>


        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Other activities</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle"><img src="images/180dcm.jpg"></td>
            <td width="55%" valign="center">
              In Munich, I was the founder and president of a student-run management consultancy for non-profits, <a href="https://www.180dcmunich.org/">180DC Munich</a>. With great interdisciplinary colleagues, we have already helped more than 30 NGOs improve their impact measurement and effectivity.
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <img src="images/int.jpg" alt="internships">
            </td>
            <td width="55%" valign="center">
              I am a curious person.
              <br>
              I got the chance to gain some valuable experiences in consulting and more recently in the technology sector, including internships at Facebook AI Research and Transferwise.
              <br>
              More to come.
            </a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Great template from <a href="https://jonbarron.info/"> Jon Barron</a>
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
