<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yuki M Asano</title>

  <meta name="author" content="Yuki M Asano">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yuki M. Asano</name>
              </p>
              <p style="text-align:center"> Computer Vision | Machine Learning | Complex Systems </p>
              <p>
                I'm an assistant professor for computer vision and machine learning at the <a href="https://ivi.fnwi.uva.nl/quva/">QUVA lab</a> at the University of Amsterdam, where I work with <a href="https://www.ceessnoek.info/">Cees Snoek</a>, <a href="https://staff.fnwi.uva.nl/m.welling/">Max Welling</a> and <a href="https://www.egavves.com/">Efstratios Gavves</a>.
                My PhD was at the <a href="https://www.robots.ox.ac.uk/~vgg/">Visual Geometry Group (VGG)</a> at the University of Oxford where I worked with <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a> and <a href="https://chrirupp.github.io/">Christian Rupprecht</a>. Prior to this I studied physics at the <a href="http://www.en.uni-muenchen.de/index.html">University of Munich (LMU)</a> and <a href="https://www.fernuni-hagen.de/">Economics in Hagen</a> as well as a <a href="https://www.maths.ox.ac.uk/members/students/postgraduate-courses/msc-mmsc">MSc in Mathematical Modelling and Scientific Computing</a> at the Mathematical Institute in Oxford. Also, I love running, the mountains ⛰️ and their combination.
              </p>
              <p style="text-align:center">
                <a href="mailto:yukiATMARKrobots.ox.ac.uk">Email</a> &nbsp/&nbsp
                <!-- <a href="data/YukiMAsano-CV.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.co.uk/citations?user=CdpLhlgAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/yukimasano/">Github</a>&nbsp/&nbsp
                <a href="https://twitter.com/y_m_asano">Twitter</a>&nbsp/&nbsp
                <a href="https://www.linkedin.com/in/yuki-m-asano">LinkedIn</a>&nbsp/&nbsp
                <a href="data/Asano_CV.pdf">CV</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/yuki.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yuki.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li>Starting as an Assistant Professor at the UvA from Oct 2021. </li>
                  <li>Two papers accepted at NeurIPS'21 (including the first as supervisor) </li>
                  <li>One paper accepted at NeurIPS'21-Datasets Track: the <a href=https://www.robots.ox.ac.uk/~vgg/research/pass/>PASS dataset</a>, incl. <a href=https://github.com/yukimasano/PASS/>pretrained models</a>. </li>
                  <li>Passed my PhD with "no corrections", my examiners were Phillip Isola and Philip Torr. </li>
                  <li>Two papers accepted to ICCV'21! (GDT and STiCA) </li>
                  <li>One paper I supervised accepted to ACL'21's Workshop on online abuse and harms. More details to follow. </li>
                  <li>One paper accepted to <a href=https://www.pnas.org/content/118/27/e2025721118>PNAS</a>, my Erdös Number is now 3 via Jobst Heitzig. </li>
                  <li>New preprint: using clustering & contrastive SSL we find objects <a href=https://arxiv.org/abs/2104.06401>without any supervision </a> </li>
                  <li>OxAI team I supervised has published its results at ICLR'21 <a href=https://sdg-quality-privacy-bias.github.io/papers/>SDG workshop  </a> </li>
                  <li>Our new preprint on intersectional occupational biases of GPT-2 is <a href=http://arxiv.org/abs/2102.04130>out </a> </li>
                  <li>Our paper on video-text representation learning got accepted as a Spotlight into ICLR 2021!</li>
                  <li>I've started volunteering my time at OxAI to help interdisciplinary teams work on AI projects.</li>
                  <li>Our paper on Self-Labelling Videos (SeLaVi) was accepted as a paper to NeurIPS! <a href="https://github.com/facebookresearch/selavi">Code</a> </li>
                  <li>Starting my summer internship June 22nd  at FAIR and working with Armand Joulin and Ishan Misra. </li>
                  <li>I am Co-PI on a Amazon Machine Learning Award project with Christian Rupprecht and Andrea Vedaldi. </li>
                  <!-- <li>Awarded with the 2020 <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/2020-europe"> Qualcomm Innovation Fellowship</a>.</li> -->
                  <!-- <li>I'll be co-organizing a workshop at ECCV 2020: <a href="https://sslwin.org/">Self-Supervised Learning: What Is Next?</a> </li> -->
                  <!-- <li>Two papers have been accepted into ICLR 2020 (incl. one Spotlight)</li> -->
                  <!-- <li>Started using a new homepage!</li> -->
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, self-supervised learning and multi-modal learning.
                More specifically, I want to understand the necessity and scope of prior knowledge and supervision for good neural networks. To this effect, I work with self-supervised learning and try to understand what makes things work and how far we can go without labels. I'm excited about what we can learn from data alone, from data augmentation, and videos.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/motionformer.png" alt="trajectory attention" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2106.05392">
                <papertitle>Keeping Your Eye On the Ball: Trajectory Attention in Video Transformers</papertitle>
              </a>
              <br>
              <a href="https://mandelapatrick.github.io/">Mandela Patrick*</a>, <a href="https://sites.google.com/view/djcampbell">Dylan Campbell*</a>, <a href="https://yukimasano.github.io/"><strong>Yuki M. Asano</strong>*</a>, <a href="https://imisra.github.io/">Ishan Misra</a>, <a href="https://www.cs.cmu.edu/~fmetze/interACT/Home.html">Florian Metze</a>, <a href="https://feichtenhofer.github.io/">Christoph Feichtenhofer</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>, <a href="http://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>
              <br>
              <em>NeurIPS</em>, 2021 &nbsp <font color=#FF8080><strong>(Oral)</strong></font>
              <br>
              <a href="https://github.com/facebookresearch/Motionformer">code | <a href="data/patrick2021motionformer.bib">bibtex</a>
              <p>We present <i>trajectory attention</i>, a drop-in self-attention block for video transformers that implicitly tracks space-time patches along motion paths. We set SOTA results on a number of action recognition datasets: Kinetics-400, Something-Something V2, and Epic-Kitchens.
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gpt-2.png" alt="predictions vs ground-truth (US)" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2102.04130">
                <papertitle>Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models </papertitle>
              </a>
              <br>
              <a href=https://www.hannahrosekirk.com/>Hannah Kirk </a>, Yennie Jun, Haider Iqbal, Elias Benussi, Filippo Volpin, Frederic A. Dreyer, Aleksandar Shtedritski, <strong>Yuki M. Asano</strong>
              <br>
              <em>NeurIPS</em>, 2021 &nbsp
              <br>
              <a href="https://github.com/oxai/intersectional_gpt2">code | <a href="data/kirk2021bias.bib">bibtex</a>
              <p>We analyze the biases and distributions of GPT-2's output w.r.t. to occupations. Especially interesting as AI find its way into hiring and automated application assessments.
            </td>
           </tr>
           <tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pass.png" alt="the pass dataset" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/forum?id=BwzYI-KaHdr">
                <papertitle>PASS: An ImageNet replacement for self-supervised pretraining without humans.</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano</strong>, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>, <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>NeurIPS Datasets and Benchmarks</em>, 2021 &nbsp </font>
              <br>
              <a href="https://www.robots.ox.ac.uk/~vgg/research/pass/">webpage | <a href="https://zenodo.org/record/5501843">data</a> | <a href="data/asano2021pass.bib">bibtex</a> | <a href="https://github.com/yukimasano/PASS/">pretrained models</a>
              <p>We introduce PASS, a large-scale image dataset that does not include any humans, and show that it can be used for high-quality model pretraning while significantly reducing privacy concerns.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/stica.png" alt="crops help training speed" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2103.10211">
                <papertitle>Space-Time Crop & Attend: Improving Cross-modal Video Representation Learning.</papertitle>
              </a>
              <br>
              Mandela Patrick*<strong>, Yuki M. Asano*</strong>, <a href="https://scholar.google.com/citations?user=JdEJEicAAAAJ&hl=en&oi=ao">Bernie Huang*</a>, Ishan Misra, Florian Metze, <a href="http://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICCV</em>, 2021 &nbsp </font>
              <br>
              <a href="https://github.com/facebookresearch/GDT">code | <a href="data/patrick2021spacetime.bib">bibtex</a>
              <p>We better leverage latent time and space for video representation learning by computing efficient multi-crops in embedding space and using a shallow transformer to model time. This yields SOTA performance and allows for training with longer videos.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gdt.png" alt="hierarchical transformations" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2003.04298">
                <papertitle>On Compositions of Transformations in Contrastive Self-Supervised Learning</papertitle>
              </a>
              <br>
              Mandela Patrick*<strong>, Yuki M. Asano*</strong>, <a href="https://scholar.google.com/citations?user=JdEJEicAAAAJ&hl=en&oi=ao">Polina Kuznetsova</a>, <a href="http://ruthcfong.github.io/">Ruth Fong</a>, <a href="http://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>, Geoffrey Zweig, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICCV</em>, 2021 &nbsp </font>
              <br>
              <a href="https://github.com/facebookresearch/GDT">code | <a href="data/patrick2020multimodal.bib">bibtex</a>
              <p>We give transformations the prominence they deserve by introducing a systematic framework suitable for contrastive learning. SOTA video representation learning by learning (in)variances systematically.
            </td>
          </tr>


           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ramsey.png" alt="ramsey model" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.pnas.org/content/118/27/e2025721118">
                <papertitle>Emergent inequality and business cycles in a simple behavioral macroeconomic model</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano</strong>, <a href="https://www.pik-potsdam.de/members/kolb">Jakob J. Kolb</a>, <a href="https://www.pik-potsdam.de/members/heitzig">Jobst Heitzig</a>, <a href="https://www.inet.ox.ac.uk/people/j-doyne-farmer/">J. Doyne Farmer</a>
              <br>
              <em>Proceedings of the National Academy of Sciences (PNAS)</em>, 2021
              <br>
              <a href="https://github.com/yukimasano/rck_abm">code | <a href="data/asano2021emergent.bib">bibtex</a>
              <p>We build an agent-based version of a fundamental macroeconomic model and include simple decision making heuristics. We find highly complex behavior and business cycles.</tr>
          </tr>

           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/avdet.jpg" alt="Detecting objects without supervision" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2104.06401">
                <papertitle>Self-supervised object detection from audio-visual correspondence</papertitle>
              </a>
              <br>
              <a href=https://www.robots.ox.ac.uk/~afourast/>Triantafyllos Afouras* </a>, <strong>Yuki M. Asano*</strong>, Francois Fagan, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>, Florian Metze
              <br>
              <a href="data/afouras2021selfsupervised.bib">bibtex</a>
              <p>We detect objects without any supervisory signal by leveraging multi-modal signals from videos and combining self-supervised contrastive- and clustering-based learning. Our model learns from video and detects objects in images.
            </td>
           </tr>
           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/privacy.png" alt="Schematic of our method" width="160" height="160">
            </td>
              <td width="75%" valign="middle">
              <a href="https://sdg-quality-privacy-bias.github.io/papers/SDG_paper_4.pdf">
                 <papertitle>Privacy-preserving object detection</papertitle>
              </a>
              <br>
              Peiyang He, Charlie Griffin, Krzysztof Kacprzyk, Artjom Joosen, Michael Collyer, Aleksandar Shtedritski, <strong>Yuki M. Asano</strong>
              <br>
              <em>ICLR</em>, 2021 SGD workshop
              <br>
              <a href="data/he2021privacypreserving.bib">bibtex</a>
              <p>We evaluate the potential of conducting object detection with blurred and GAN-swapped faces. It works well and can potentially even alleviate biases.
            </td>
          </tr>


            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/support_set.png" alt="Schematic of our method" width="160" height="160">
            </td>
              <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2010.02824">
                 <papertitle>Support-set bottlenecks for video-text representation learning</papertitle>
              </a>
              <br>
              Mandela Patrick*, <a href="http://www.cs.cmu.edu/~poyaoh/">Po-Yao Huang*</a>, <strong>Yuki M. Asano*</strong>, Florian Metze, Alexander Hauptmann, <a href="http://www.robots.ox.ac.uk/~joao/">João F. Henriques</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICLR</em>, 2021 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
              <br>
              <a href="data/patrick2020supportset.bib">bibtex</a> | <a href="https://slideslive.com/38953567/supportset-bottlenecks-for-videotext-representation-learning?ref=speaker-42650-latest">talk</a>
              <p>We use a generative objective to improve the instance discrimination limitations of contrastive learning to set new state-of-the-art results in text-to-video retrieval.
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/labelling_videos.gif" alt="clustered videos" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="http://arxiv.org/abs/2006.13662">
                <papertitle>Labelling unlabelled videos from scratch with multi-modal self-supervision</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano*</strong>, Mandela Patrick*, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>NeurIPS</em>, 2020
              <br>
               <a href="https://github.com/facebookresearch/selavi">code |</a> <a href="https://www.robots.ox.ac.uk/~vgg/research/selavi/">homepage |</a> <a href="data/asano2020labelling.bib">bibtex</a> | <a href="https://slideslive.com/38955434/selavi-selflabelling-videos-without-any-annotations-from-scratch?ref=account-84503-latest"> talk</a>
              <p>Unsupervisedly clustering videos via self-supervision. We show clustering videos well does not come for free from good representations. Instead, we learn a multi-modal clustering function that treats the audio and visual-stream as augmentations.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/self-clusters.jpg" alt="learned clusters" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/research/self-label/asset/iclr2020_self-label.pdf">
                <papertitle>Self-labelling via simultaneous clustering and representation learning</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano</strong>, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICLR</em>, 2020 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
              <br>
              <a href="https://github.com/yukimasano/self-label">code | </a> <a href="http://www.robots.ox.ac.uk/~vgg/blog/self-labelling-via-simultaneous-clustering-and-representation-learning.html">blog | </a> <a href="data/asano2020self.bib">bibtex</a> | <a href="https://iclr.cc/virtual_2020/poster_Hyx-jyBFPr.html">ICLR talk</a>
              <p>We propose a self-supervised learning formulation that simultaneously learns feature representations and useful dataset labels by optimizing the common cross-entropy loss for features <i>  and </i> labels, while maximizing information.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ameyoko_crop.jpg" alt="ameyoko" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.13132">
                <papertitle>A critical analysis of self-supervision, or what we can learn from a single image</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano</strong>, <a href="https://chrirupp.github.io/">Christian Rupprecht</a>, <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
              <br>
              <em>ICLR</em>, 2020
              <br>
              <a href="data/asano2020a.bib">bibtex</a> | <a href="https://github.com/yukimasano/single_img_pretraining">code</a> | <a href="https://iclr.cc/virtual_2020/poster_B1esx6EYvr.html">ICLR talk</a>
              <p>We evaluate self-supervised feature learning methods and find that with sufficient data augmentation early layers can be learned using just one image.  This is informative about self-supervision and the role of augmentations.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/recipes.png" alt="recipes" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.nature.com/articles/s41893-019-0316-0">
                <papertitle>Rising adoption and retention of meat-free diets in online recipe data</papertitle>
              </a>
              <br>
              <strong>Yuki M. Asano*</strong> and  <a href="https://www.cdtm.de/cdtm_team/gesa-biermann/">Gesa Biermann*</a>
              <br>
              <em>Nature Sustainability </em>, 2019
              <br>
              <a href="https://github.com/yukimasano/sustainable_recipes">code</a> | <a href="data/asano2019rising.bib">bibtex</a>
              <p>We investigate dietary transitions by analysing a large scale dataset of recipes and user ratings. We detect a consistent increase in the number of users switching to vegetarian diets, and maintaining them. We show that the transition is eased by initially switching to vegetarian diets  </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/protonCT.jpg" alt="protonCT" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://aapm.onlinelibrary.wiley.com/doi/abs/10.1118/1.4924232">
              <papertitle>Monte Carlo Study of the Precision and Accuracy of Proton CT Reconstructed Relative Stopping Power Maps</papertitle>
              </a>
              <br> G. Dedes, <strong>YM. Asano</strong>, N. Arbor, D. Dauvergne, J. Letang, E. Testa, S. Rit, K. Parodi
              <br>
              <em>Medical Physics</em>, 2016
              <br>
              <a href="data/dedes2015j.bib">bibtex</a>
              <p> In my BSc thesis, I investigated how we can model proton computation tomography (pCT) using Monte-Carlo based software. We simulated an ideal pCT scanner and scans of several cylindrical phantoms with various tissue equivalent inserts of different sizes.
            </td>
          </tr>


        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Other activities</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle"><img src="images/180dcm.jpg"></td>
            <td width="55%" valign="center">
              In Munich, I was the founder and president of a student-run management consultancy for non-profits, <a href="https://www.180dcmunich.org/">180DC Munich</a>. With great interdisciplinary colleagues, we have already helped more than 30 NGOs improve their impact measurement and effectivity.
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <img src="images/int.jpg" alt="internships">
            </td>
            <td width="55%" valign="center">
              I am a curious person.
              <br>
              I got the chance to gain some valuable experiences in consulting and more recently in the technology sector, including internships at Facebook AI Research and Transferwise.
              <br>
              More to come.
            </a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Great template from <a href="https://jonbarron.info/"> Jon Barron</a>
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
